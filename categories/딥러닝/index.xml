<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>딥러닝 - Category - brog</title>
        <link>http://localhost:1313/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/</link>
        <description>딥러닝 - Category - brog</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Nov 2024 21:43:30 &#43;0900</lastBuildDate><atom:link href="http://localhost:1313/categories/%EB%94%A5%EB%9F%AC%EB%8B%9D/" rel="self" type="application/rss+xml" /><item>
    <title>Entropy</title>
    <link>http://localhost:1313/posts/entropy/</link>
    <pubDate>Wed, 27 Nov 2024 21:43:30 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/entropy/</guid>
    <description><![CDATA[ensembleEnsemble $X=(x,A_x,P_x)$ outcome $x$ set of possible values $A_x=\{a_1,a_2,...a_i,a_I\}$ , 각각의 확률 $P_X=\{p_1,p_2,...,p_I\}$ abbreviation : $P(x=a_i)$ 를 $P(a_i),P(x)$ 로 씀
probability of subsetT가 $A_X$의 subset이면, $P(T)=P(x\in T)=\sum\limits_{a_i\in T}P(x=a_i)$
joint ensemblejoint ensemble $XY$ : outcome이 순서쌍 $x,y$ joint probability : $P(x,y)$ &lt;- 쉼표 안붙여도 됨! $xy<=>x,y$ 확률변수 X,Y가 독립일 필요는 없다.
marginal probabilityjoint probability에서 한 확률변수에 대한 summation으로 marginal probability를 얻을 수 있다 $$P(x=a_i)=\sum\limits_{y\in A_Y}P(x=a_i,y)$$ conditional probability$$P(x=a_i|y=b_j)=\frac{P(x=a_i,y=b_j)}{P(y=b_{j})}$$ ==Chain rule (Product rule)==$$P(x,y|H)=P(x|y,H)P(y|H)=P(y|x,H)P(x|H)$$ Sum rule$$P(x|H)=\sum\limits_yP(x,y|H)=\sum\limits_yP(x|y,H)P(y|H)$$ Bayes&rsquo; Theorem$$P(y|x,H)=\frac{P(x|y,H)P(y|H)}{P(x|h)}$$ Independencer.]]></description>
</item><item>
    <title>딥러닝의 Cross Entropy Loss 이해하기(w/ negative log likelihood) (1)</title>
    <link>http://localhost:1313/posts/cross-entropy/</link>
    <pubDate>Wed, 27 Nov 2024 19:48:42 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/cross-entropy/</guid>
    <description><![CDATA[딥러닝에서 자주 사용되는 Cross Entropy Loss를 알아봅시다! 정보이론에서 등장하는 개념인 cross entropy가 왜 딥러닝의 loss로 활용되는지, 또 통계학의 negative log likelihood와는 어떤 관계가 있는지에 대해 이해하고자 쓰는 글입니다.]]></description>
</item><item>
    <title>생성모델 평가지표 IS, FID, LPIPS</title>
    <link>http://localhost:1313/posts/generative-models-evaluation/</link>
    <pubDate>Wed, 07 Aug 2024 19:35:06 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/generative-models-evaluation/</guid>
    <description><![CDATA[생성모델의 평가지표 IS, FID, LPIPS 에 대해 알아보자!]]></description>
</item><item>
    <title>VAE 이해해보기</title>
    <link>http://localhost:1313/posts/vae/</link>
    <pubDate>Wed, 31 Jul 2024 21:22:15 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/vae/</guid>
    <description><![CDATA[이 글은 VAE 논문(Auto-Encoding Variational Bayes)을 리뷰하다가 막혀서 오토인코더의 모든 것, 딥러닝 VAE 를 참고해서 VAE를 이해 + 수식적인 부분 정리를 목적으로 쓴 글입니다.
0. Background AE vs VAE?통계와 수학으로 가득한 내용에 걸맞게 논문의 제목이 Auto-Encoding Variational Bayes인 것과 달리, 우리에게 흔히 알려져 있는 이름은 VAE(variational autoencoder)이다. 그렇다면 이 논문은 autoencoder와는 어떤 관계에 있는 것일까?
AE의 구조 Autoencoder는 인코더-디코더로 이루어진 네트워크이다. 입출력이 동일한 네트워크로, 인코더는 앞단에서 original data를 input으로 받아 z 차원의 latent를 output으로 뱉고, 디코더는 뒷단에서 latent를 input으로 받아 다시 original data와 같은 차원의 output을 출력한다.]]></description>
</item></channel>
</rss>
