<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Braveseokyung - Author - brog</title>
        <link>http://localhost:1313/authors/braveseokyung/</link>
        <description>Braveseokyung - Author - brog</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 27 Nov 2024 21:43:30 &#43;0900</lastBuildDate><atom:link href="http://localhost:1313/authors/braveseokyung/" rel="self" type="application/rss+xml" /><item>
    <title>Entropy</title>
    <link>http://localhost:1313/posts/entropy/</link>
    <pubDate>Wed, 27 Nov 2024 21:43:30 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/entropy/</guid>
    <description><![CDATA[ensembleEnsemble $X=(x,A_x,P_x)$ outcome $x$ set of possible values $A_x=\{a_1,a_2,...a_i,a_I\}$ , 각각의 확률 $P_X=\{p_1,p_2,...,p_I\}$ abbreviation : $P(x=a_i)$ 를 $P(a_i),P(x)$ 로 씀
probability of subsetT가 $A_X$의 subset이면, $P(T)=P(x\in T)=\sum\limits_{a_i\in T}P(x=a_i)$
joint ensemblejoint ensemble $XY$ : outcome이 순서쌍 $x,y$ joint probability : $P(x,y)$ &lt;- 쉼표 안붙여도 됨! $xy<=>x,y$ 확률변수 X,Y가 독립일 필요는 없다.
marginal probabilityjoint probability에서 한 확률변수에 대한 summation으로 marginal probability를 얻을 수 있다 $$P(x=a_i)=\sum\limits_{y\in A_Y}P(x=a_i,y)$$ conditional probability$$P(x=a_i|y=b_j)=\frac{P(x=a_i,y=b_j)}{P(y=b_{j})}$$ ==Chain rule (Product rule)==$$P(x,y|H)=P(x|y,H)P(y|H)=P(y|x,H)P(x|H)$$ Sum rule$$P(x|H)=\sum\limits_yP(x,y|H)=\sum\limits_yP(x|y,H)P(y|H)$$ Bayes&rsquo; Theorem$$P(y|x,H)=\frac{P(x|y,H)P(y|H)}{P(x|h)}$$ Independencer.]]></description>
</item><item>
    <title>딥러닝의 Cross Entropy Loss 이해하기(w/ negative log likelihood) (1)</title>
    <link>http://localhost:1313/posts/cross-entropy/</link>
    <pubDate>Wed, 27 Nov 2024 19:48:42 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/cross-entropy/</guid>
    <description><![CDATA[딥러닝에서 자주 사용되는 Cross Entropy Loss를 알아봅시다! 정보이론에서 등장하는 개념인 cross entropy가 왜 딥러닝의 loss로 활용되는지, 또 통계학의 negative log likelihood와는 어떤 관계가 있는지에 대해 이해하고자 쓰는 글입니다.]]></description>
</item><item>
    <title>Information Theory의 관점에서 Estimator 살펴보기</title>
    <link>http://localhost:1313/posts/estimators/</link>
    <pubDate>Wed, 20 Nov 2024 17:54:51 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/estimators/</guid>
    <description><![CDATA[Information theory의 관점에서 Estimator를 살펴보자. 키워드 - MSE, MVUE, Fisher Information, CRLB]]></description>
</item><item>
    <title>[논문 리뷰] Emergent Visual-Semantic Hierarchies in Image-Text Representation</title>
    <link>http://localhost:1313/posts/hierarcaps/</link>
    <pubDate>Wed, 06 Nov 2024 20:55:46 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/hierarcaps/</guid>
    <description><![CDATA[ECCV 2024 oral paper인 Emergent Visual-Semantic Hierarchies in Image-Text Representation 논문에 대한 리뷰입니다.]]></description>
</item><item>
    <title>네이버 쇼핑 크롤링 (Selenium, 네이버 API)</title>
    <link>http://localhost:1313/posts/naver-shopping-crawling/</link>
    <pubDate>Wed, 02 Oct 2024 11:39:16 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/naver-shopping-crawling/</guid>
    <description><![CDATA[네이버 쇼핑에서 상품을 검색한 결과를 크롤링하는 두 가지 방법을 소개합니다. 1) Selenium로 검색 자동화 2) 네이버 API]]></description>
</item><item>
    <title>[논문 리뷰] Accept the Modality Gap : An Exploration in Hyperbolic Space</title>
    <link>http://localhost:1313/posts/accept-the-modality-gap/</link>
    <pubDate>Tue, 24 Sep 2024 00:16:26 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/accept-the-modality-gap/</guid>
    <description><![CDATA[CVPR 2024 Highlight paper인 Accept the Modality Gap An Exploration in Hyperbolic Space 논문을 읽고 정리한 내용입니다.]]></description>
</item><item>
    <title>DNS</title>
    <link>http://localhost:1313/posts/network-dns/</link>
    <pubDate>Wed, 11 Sep 2024 13:09:06 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/network-dns/</guid>
    <description><![CDATA[도메인부터 DNS 쿼리까지 DNS에 대해 알아보자]]></description>
</item><item>
    <title>TCP와 UDP</title>
    <link>http://localhost:1313/posts/network-tcp-udp-2/</link>
    <pubDate>Tue, 03 Sep 2024 21:30:12 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/network-tcp-udp-2/</guid>
    <description><![CDATA[전송계층의 TCP와 UDP의 차이를 알아보자]]></description>
</item><item>
    <title>TCP와 UDP</title>
    <link>http://localhost:1313/posts/network-tcp-udp/</link>
    <pubDate>Wed, 28 Aug 2024 15:37:24 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/network-tcp-udp/</guid>
    <description><![CDATA[전송계층의 TCP와 UDP의 차이를 알아보자]]></description>
</item><item>
    <title>Bayesian Estimator</title>
    <link>http://localhost:1313/posts/9-bayesian-estimation/</link>
    <pubDate>Wed, 21 Aug 2024 21:00:37 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>http://localhost:1313/posts/9-bayesian-estimation/</guid>
    <description><![CDATA[MLE(Maximum Likelihood Estimator)와는 다른, Bayesian Estimator에 대해 알아보자!]]></description>
</item></channel>
</rss>
