<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Generative-Models - Series - brog</title>
        <link>https://braveseokyung.github.io/series/generative-models/</link>
        <description>Generative-Models - Series - brog</description>
        <generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Wed, 31 Jul 2024 21:22:15 &#43;0900</lastBuildDate><atom:link href="https://braveseokyung.github.io/series/generative-models/" rel="self" type="application/rss+xml" /><item>
    <title>VAE 이해해보기</title>
    <link>https://braveseokyung.github.io/posts/vae/</link>
    <pubDate>Wed, 31 Jul 2024 21:22:15 &#43;0900</pubDate><author>
                    <name>braveseokyung</name>
                </author><guid>https://braveseokyung.github.io/posts/vae/</guid>
    <description><![CDATA[이 글은 VAE 논문(Auto-Encoding Variational Bayes)을 리뷰하다가 막혀서 오토인코더의 모든 것, 딥러닝 VAE 를 참고해서 VAE를 이해 + 수식적인 부분 정리를 목적으로 쓴 글입니다.
0. Background AE vs VAE?통계와 수학으로 가득한 내용에 걸맞게 논문의 제목이 Auto-Encoding Variational Bayes인 것과 달리, 우리에게 흔히 알려져 있는 이름은 VAE(variational autoencoder)이다. 그렇다면 이 논문은 autoencoder와는 어떤 관계에 있는 것일까?
AE의 구조 Autoencoder는 인코더-디코더로 이루어진 네트워크이다. 입출력이 동일한 네트워크로, 인코더는 앞단에서 original data를 input으로 받아 z 차원의 latent를 output으로 뱉고, 디코더는 뒷단에서 latent를 input으로 받아 다시 original data와 같은 차원의 output을 출력한다.]]></description>
</item></channel>
</rss>
